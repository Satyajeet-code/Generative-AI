{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/Satyajeet-code/Generative-AI/blob/main/RAGSession42AI/RAGFusion.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMQHwnO0g1Iz",
    "outputId": "d57f83d5-0835-4514-f0a5-edf824b1b6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install langchain faiss-cpu langchain-groq  langchain-community chromadb pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "_YQelIwagak0"
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import requests\n",
    "import re\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "4TY9Bhxqg8Cg"
   },
   "outputs": [],
   "source": [
    "groq=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "bhvXdNdOiMLq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "MBjDqi0hg__0"
   },
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(r\"/content/Applying generative AI with retrieval augmented generation to summarize and extract key clinical infor.pdf\")\n",
    "docs = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "OVZ8T2J4hDOI"
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "ht_lXLm6hOVQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "texts = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "Igc-04dbhTyE"
   },
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "embedding_function = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "oQxceXkIha_X"
   },
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts,\n",
    "                       embedding_function,\n",
    "                       persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAcO3nEJhzXX",
    "outputId": "63a39dd3-bc60-4187-9c02-72e9123878e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 1, 'source': '/content/Applying generative AI with retrieval augmented generation to summarize and extract key clinical infor.pdf'}, page_content='This technique combines elements of both retrieval and generation \\nmethods [38]. In this approach, the model uses a retrieval system to \\nretrieve relevant information from a dataset or knowledge base and then \\ngenerates responses or content based on that retrieved information [39]. \\nRetrieval systems are efficient in finding relevant information from a \\nlarge dataset, while generation models excel at creating coherent and \\ncontextually appropriate responses. By incorporating retrieval, the'),\n",
       " Document(metadata={'source': 'text_file.txt'}, page_content='^ Nanda, Neel; Chan, Lawrence; Lieberum, Tom; Smith, Jess; Steinhardt, Jacob (2023-01-01). \"Progress measures for grokking via mechanistic interpretability\". arXiv:2301.05217 [cs.LG].\\n^ a b c d e Mitchell, Melanie; Krakauer, David C. (28 March 2023). \"The debate over understanding in AI\\'s large language models\". Proceedings of the National Academy of Sciences. 120 (13): e2215907120. arXiv:2210.13966. Bibcode:2023PNAS..12015907M. doi:10.1073/pnas.2215907120. PMC\\xa010068812. PMID\\xa036943882.'),\n",
       " Document(metadata={'page': 3, 'source': '/content/Applying generative AI with retrieval augmented generation to summarize and extract key clinical infor.pdf'}, page_content='retrieved documents are sent to the generative model along with specific \\nprompt instruction to produce a concise and informative summary \\n( Fig. 2 ). \\nWe chose Llama 2 model since it is the leading open-source model. \\nGiven that Llama 2 was regarded as the best available model for the task, \\nespecially considering its origins from a reputable research team like \\nMeta AI, it was selected as the primary model for the study. This decision'),\n",
       " Document(metadata={'source': 'text_file.txt'}, page_content='A simpler form of tool use is retrieval-augmented generation: the augmentation of an LLM with document retrieval. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Explain the methodology\"\n",
    "\n",
    "db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "3G_o4JtAiIoH"
   },
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n",
    "llm =ChatGroq(groq_api_key=groq, model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "7sGxx21mjTZn"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the following question based only on the provided context.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <Question>\n",
    "    {input}\n",
    "    </Question>\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "output = response[\"answer\"]\n",
    "output = output.replace(\"*\", \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "id": "bexyY0eCjedk",
    "outputId": "bfe62255-9b9b-41d4-9fd6-5a7215794d47"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'According to the provided context, the methodology involves a combination of retrieval and generation methods. Here\\'s a step-by-step explanation:\\n\\n1. The model uses a retrieval system to retrieve relevant information from a dataset or knowledge base.\\n2. The retrieved documents are sent to the generative model along with specific prompt instructions.\\n3. The generative model produces a concise and informative summary based on the retrieved documents and the prompt instructions.\\n\\nThis methodology is referred to as \"retrieval-augmented generation\", where a document retriever is used to retrieve relevant documents for a given query, and then an LLM (Large Language Model) generates an output based on both the query and the context included from the retrieved documents.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "HtNxK2b6jmAn"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate, PromptTemplate\n",
    "prompt = ChatPromptTemplate(input_variables=['query'],\n",
    "                            messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[],template='You are a helpful assistant that generates multiple search queries based on a single input query.')),\n",
    "                            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='Generate multiple search queries related to: {question} \\n OUTPUT (2 queries):'))])\n",
    "\n",
    "\n",
    "generate_queries = (\n",
    "    prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "uuNRN9jWmOC3"
   },
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=1):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "DG7vZo6at5mw"
   },
   "outputs": [],
   "source": [
    "ragfusion_chain = generate_queries | retriever.map() | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "jWilmx1huHT1",
    "outputId": "18de754e-c28e-4af2-bed6-f0c99e2f861f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Explain the methodology'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "up0RmCqJuLdA"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "full_rag_fusion_chain = (\n",
    "    {\n",
    "        \"context\": ragfusion_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWcbvtMavnIq",
    "outputId": "5b162ce7-9c68-4a03-9375-1c95acfa098c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The methodology used in this research involves the application of generative AI with retrieval-augmented generation to summarize and extract key clinical information. The approach combines elements of both retrieval and generation methods, where a retrieval system is used to retrieve relevant information from a dataset or knowledge base and then generates responses or content based on that retrieved information.\\n\\nThe methodology involves the following steps:\\n\\n1. Retrieval: The retrieval system uses a search algorithm to retrieve related documents from a dataset or knowledge base.\\n2. Generation: The generated model uses the retrieved documents to generate a summary or response.\\n3. Post-processing: The generated summary or response is then post-processed to ensure its coherence and relevance to the query.\\n\\nThe methodology also involves the use of a parameter for the search operation (k) which was set at 20, which is the limitation of the available GPU memory for processing documents simultaneously without triggering out-of-memory errors.\\n\\nOverall, the methodology aims to leverage the strengths of both retrieval and generation methods to improve the accuracy and relevance of the generated summaries or responses.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 211, 'prompt_tokens': 2230, 'total_tokens': 2441, 'completion_time': 0.175833333, 'prompt_time': 0.110845272, 'queue_time': 0.0018446870000000115, 'total_time': 0.286678605}, 'model_name': 'Llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-4875e818-6391-4e89-ae0e-766ad1c82b59-0', usage_metadata={'input_tokens': 2230, 'output_tokens': 211, 'total_tokens': 2441})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_rag_fusion_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "8C7LqAyT3MXZ",
    "outputId": "9ad6191d-e759-486e-8204-2d976d0d6768"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The methodology described in the provided context is the use of Retrieval-Augmented Generation (RAG) for summarization and extraction of key clinical information. This approach combines the strengths of both retrieval and generation methods.\\n\\nThe methodology involves the following steps:\\n\\n1. Retrieval: The model uses a retrieval system to retrieve relevant information from a dataset or knowledge base. This is done by encoding the query and the documents into vectors, and then finding the documents with vectors most similar to the vector of the query.\\n2. Generation: The retrieved documents are sent to a generative model along with specific prompt instructions to produce a concise and informative summary.\\n3. Processing: The model breaks the nursing notes and structured data into manageable chunks of a fixed size (600 characters) to facilitate subsequent processing and analysis.\\n\\nThe authors also mention the use of a specific model, Llama 2, which is the leading open-source model and was chosen for the study due to its origins from a reputable research team like Meta AI.\\n\\nThe methodology is described in the context of applying generative AI with retrieval-augmented generation to summarize and extract key clinical information from nursing notes and structured data. The authors aim to enable the model to access an external knowledge base, including more nursing notes and structured data, to improve the accuracy and completeness of the summaries and extracted information.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_rag_fusion_chain.invoke({\"question\": query}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAxTaOKK4HSC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
