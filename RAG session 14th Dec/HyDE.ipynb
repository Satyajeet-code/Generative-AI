{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/Satyajeet-code/Generative-AI/edit/main/RAG%20session%2014th%20Dec/HyDE.ipynb)\n"
      ],
      "metadata": {
        "id": "2K5oUwUjOWA0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDeeo-auAZNR",
        "outputId": "3d6c1675-9045-4af3-9e3a-6c0380f4fc7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/298.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m297.0/298.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain faiss-cpu langchain-groq  langchain-community pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.chains import LLMChain, HypotheticalDocumentEmbedder\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import langchain"
      ],
      "metadata": {
        "id": "d1KmnFsSAdAm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groq=\"\""
      ],
      "metadata": {
        "id": "RFX86V8oApUE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(r\"/content/Applying generative AI with retrieval augmented generation to summarize and extract key clinical infor.pdf\")\n",
        "docs = loader.load_and_split()"
      ],
      "metadata": {
        "id": "uWpU7bqKAsyx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"BAAI/bge-small-en-v1.5\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "NKqM2CSLAu5P"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db=FAISS.from_documents(docs,bge_embeddings)"
      ],
      "metadata": {
        "id": "RTBZgcE4LYg9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatGroq(groq_api_key=groq, model_name=\"Llama3-8b-8192\")"
      ],
      "metadata": {
        "id": "PXI8FtLQAxxd"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.debug=False"
      ],
      "metadata": {
        "id": "-Q_6ooKWKZV_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"LLm Evaluation\"\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "  (\"system\", \"Please answer the query asked by the user. Keep it short and detailed.\"),\n",
        "  (\"user\", f\"Question:{query}\")\n",
        "])\n",
        "\n",
        "chain=prompt|llm\n",
        "print(\"------------------------- Answer ---------------------\")\n",
        "response=chain.invoke({'query':query})\n",
        "# print(response)\n",
        "response=response.content.replace(\"*\",\"\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "GRxJumUcEn-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3872a4d-579e-4d71-f536-fdbb528c4656"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------- Answer ---------------------\n",
            "LLM (Large Language Model) Evaluation typically involves assessing its performance on various tasks, such as:\n",
            "\n",
            "1. Language Understanding: Measuring the model's ability to comprehend natural language, including sentiment analysis, entity recognition, and question answering.\n",
            "2. Language Generation: Evaluating the model's ability to generate coherent and relevant text, such as text summarization, language translation, and chatbot responses.\n",
            "3. Common Sense and Reasoning: Testing the model's ability to apply common sense and reason, such as solving puzzles, making inferences, and understanding cause-and-effect relationships.\n",
            "4. Dialogue and Conversational Flow: Assessing the model's ability to engage in natural-sounding conversations, including turn-taking, tone, and nuance.\n",
            "5. Error Analysis: Identifying and analyzing errors, such as language-specific quirks, cultural biases, and domain-specific knowledge gaps.\n",
            "\n",
            "Some popular evaluation metrics for LLMs include:\n",
            "\n",
            "\n",
            " BLEU (Block-based Evaluation of Automatic Translation)\n",
            "\n",
            " ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
            "\n",
            " METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
            "\n",
            " F1-score (measuring precision and recall)\n",
            "\n",
            " Perplexity (measuring the model's ability to predict the next word in a sequence)\n",
            "\n",
            "These metrics help developers refine and improve the performance of LLMs, enabling them to better understand and generate human-like language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=db.as_retriever()"
      ],
      "metadata": {
        "id": "I4ukcb7PLHu2"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_docs=retriever.get_relevant_documents(response)"
      ],
      "metadata": {
        "id": "Ch2pBRQtLOYn"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "collapsed": true,
        "id": "dH7SVWSTLtny",
        "outputId": "c6da4f75-b439-4c8d-df01-468c087d7dbf"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rating RAG can mitigate hallucination by providing access to more data, \\nincluding structured data. However, this approach doesn’t fundamen -\\ntally solve the underlying issue within the generative model. When ac -\\ncess to external data was revoked, the model reverted to incorrect \\noutputs, indicating a reliance on that information. Another concern is \\ninconsistency, as the model fails to produce consistent output when \\npresented with minor variations in the prompt. This observation is \\nconsistent with what others have reported [49,56]. Moreover, even with \\nthe same prompt, the model’s output varies from one note to another \\nbased on the information provided in each note. However, we found that \\ngiving the model a template to follow mitigated this issue. Alternative \\nprompting methods such as few-shot learning, chain of thoughts or fine- \\ntuning may address zero-shot learning limitations. These methods allow \\nthe model to learn from a few examples and refine its understanding and \\nreasoning, which could potentially enhance the model’s ability to \\ngenerate more logical output while minimizing hallucinations [57,58]. \\nIn this study we only explored the zero-shot strategy for its \\nsimplicity. It is likely that other prompt engineering techniques could \\nfurther improve the model performance. Incorporating few-shot exam -\\nples into model generation is challenging for two reasons. First, the \\nlength of the notes posed a significant constraint, given the model’s \\nmaximum length of 4096 tokens. Additionally, reliance on few-shot \\nexamples led to occasional errors in certain instances, making it diffi -\\ncult to create few-shot instructions that adequately cover all possible \\nsituations [59]. This struggle with contextual understanding and \\ngeneralization suggests that the model may have memorized the specific \\nexamples provided, leading to a failure in its ability to generalize \\neffectively across diverse contexts. To overcome this problem, the LLM \\nmight benefit from more explicit instructions through fine-tuning. \\nMoreover, we used the smaller Llama 2 model (13B), which, as ex -\\npected, exhibits lower performance than the larger Llama model with a \\nhigher number of parameters, e.g., Llama 2 70B [45]. Moreover, our \\nexploration was limited to just one aspect of the RAG approach. It is \\nnoteworthy that the functionalities of a RAG system extend beyond what \\nwe have demonstrated here. There are diverse RAG capabilities and \\napplications, such as creating chatbots for medical professionals and \\ndeveloping agents for various medical purposes [60]. \\nDespite the prompting technique choice and LLM’s smaller size, we \\nwere able to successfully replicate the results achieved in our previous \\nwork, which took weeks to complete [42]. The replication was achieved \\nin just a few days, with a reasonable level of accuracy. The efficiency \\ngains and practicality for use in real-world applications make this \\nexploration worthwhile. \\n5. Conclusion \\nThe latest advancements in large language models have opened \\nmany opportunities for health informatics in the near future [30]. This \\nstudy demonstrates that by utilizing LLM and generative AI, we can \\nsuccessfully extract nutrition summaries and identify risk factors for \\nmalnutrition from unstructured nursing notes within aged care EHR, a \\ntask that might otherwise be difficult to complete. It also demonstrates \\nthat the integration of the RAG approach optimizes the utilization of \\ndata by LLMs in healthcare settings. This, in turn, will improve data \\naccessibility and streamline the data analysis process. Furthermore, the \\napplication of NLP using LLMs has great potential for improving the \\nquality of care and ensuring timely interventions. It will transform the \\nway we address malnutrition and other health problems within \\nhealthcare and aged care settings. \\nCRediT authorship contribution statement \\nMohammad Alkhalaf: Writing – review & editing, Writing – orig -'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(query)[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "cSqQiNxZL7UY",
        "outputId": "f4251c48-fc72-441a-a2fd-4f5bf57298a0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3. Retrieval augmented generation \\nTo enable the model to access an external knowledge base including \\nmore nursing notes and structured data, we utilized the RAG approach. \\nThis approach involves taking nursing notes and breaking them into \\nmanageable chunks of a fixed size (600 characters). This step is essential \\nto facilitate subsequent processing and analysis. Similarly, when dealing \\nwith structured data, this approach parses the information row by row. \\nNext we utilized the sentence transformer model from Huggingface li -\\nbrary [43] . This model is designed to encode sentences into dense vec -\\ntors, capturing semantic meaning and relationships between words. By \\nembedding the segmented data, we transform information into a nu -\\nmerical representation that preserves the contextual information and \\nrelationships within the data. The embedded vectors are then stored in a \\nvector store along with their metadata, providing an efficient approach \\nfor retrieval and analysis. Vector stores allow for fast similarity searches, \\nenabling quick access to relevant information [44] . Then, we utilized \\nmaximum marginal relevance retrieval (MMR), a well-established \\nsearch algorithm provided by the LangChain library to retrieve related \\ndocuments. MMR operates by maximizing the diversity of the selected \\ndocuments while ensuring their relevance to the query. This selection \\nleverages the reliability and proven performance of MMR over alterna -\\ntive methods such as similarity search. The parameter for the search \\noperation (k) was set at 20, the limitation of the available GPU memory \\nfor processing documents simultaneously without triggering out-of- \\nmemory errors. To retrieve information for a specific resident on a \\nspecific period of time, a retriever takes a resident ID, date and a query \\nas input and searches the vector store for documents relevant to the \\nquery. The retrieved documents contain the embedded vectors repre -\\nsenting the segmented data related to the resident and query. After that, \\nretrieved documents are sent to the generative model along with specific \\nprompt instruction to produce a concise and informative summary \\n( Fig. 2 ). \\nWe chose Llama 2 model since it is the leading open-source model. \\nGiven that Llama 2 was regarded as the best available model for the task, \\nespecially considering its origins from a reputable research team like \\nMeta AI, it was selected as the primary model for the study. This decision \\nwas driven by the desire to utilize a high-quality, SOTA model from a \\nreliable source, ensuring the credibility and efficacy of the research \\noutcomes. It was also accessible for researchers to run locally, ensuring \\nthat sensitive data remains private and not shared. We also selected the \\n13 billion parameter version of the model instead of the 7 billion \\nparameter one since it did not significantly increase memory re -\\nquirements, allowing it to run efficiently on standard GPU hardware. \\nThis meant that the computational infrastructure needed for running the \\nlarger model was readily available and feasible within the project ’ s \\nresource constraints. Moreover, Meta AI ’ s tests and evaluations had \\ndemonstrated that the 13 billion parameter model exhibited superior \\nperformance compared to the 7 billion parameter version in all \\nTable 2 \\nTask 2 corpus.  \\nTask 2 Extracting malnutrition risk factors from the nurse notes \\nThis task uses zero-shot prompting to extract malnutrition risk \\nfactors from nursing notes. \\nInput Nursing note \\nOutput list of malnutrition risk factors causing malnutrition \\nNumber of \\nnotes \\nNumber of notes: 1,399 notes (550 with no risk factors mentioned.) \\nNumber of residents: 719 \\nNo. of notes per resident: Mean: 1.92, SD: 1.37.  \\nM. Alkhalaf et al.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "63d3z62wMO6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
